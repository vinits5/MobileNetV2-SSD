{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CreateDataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESABKLOlXIg3",
        "outputId": "fc7d73b1-887b-470c-ef7b-b6ddb3044f56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaEWvCkwXUSF",
        "outputId": "baede21e-72d5-43f1-86c8-cde9180854da"
      },
      "source": [
        "cd job"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'job'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTbum5OHWuIu"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy import matlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from skimage.transform import resize\n",
        "\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 10\n",
        "layerWidths = [28,14,7,4,2,1]\n",
        "numBoxes = [3,3,3,3,3,3]\n",
        "assert len(numBoxes) == len(layerWidths) # num_boxes for each layer and each layer has a specific width\n",
        "outputChannels = NUM_CLASSES + 1 + 4 # 10 classes + background + cx,cy,h,w\n",
        "assert outputChannels - NUM_CLASSES == 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7xPvmlyXATX",
        "outputId": "58185161-eb83-4a22-fb47-d3ae546d27c2"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgj0q0rXgiFH"
      },
      "source": [
        "Reference is taken from this blog. https://d2l.ai/chapter_computer-vision/anchor.html \\\\\n",
        "\"*Assume that the input image has a height of h and width of w. We generate anchor boxes with different shapes centered on each pixel of the image. Assume the size is s∈(0,1], the aspect ratio is r>0, and the width and height of the anchor box are ws√r and hs/√r, respectively. When the center position is given, an anchor box with known width and height is determined.*\" \\\\\n",
        "\n",
        "s: scale, h: grid_size, w: grid_size and r: asp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unuykcx-XyzO"
      },
      "source": [
        "def create_default_boxes():\n",
        "\t# number of scales is equal to the number of different resolutions ie num of layer widths\n",
        "\t# for a given resolution, we have different aspect ratios\n",
        "\t# num(scales) = num(layerWidth) = num(num_boxes) and num(asp_ratios) = num_boxes[i]\n",
        "\tMinScale = .1 \t\t\t\t\t# Min and Max scale given as percentage\n",
        "\tMaxScale = 1.5\n",
        "\tscales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths))]\n",
        "\tscales = scales[::-1] \t\t\t\t\t\t# reversing the order because the layer_widths go from high to low (lower to higher resoltuion)\n",
        "\n",
        "\tasp = [0.5,1.0,1.5]\n",
        "\tasp1 = [x**0.5 for x in asp]\n",
        "\tasp2 = [1/x for x in asp1]\n",
        "\n",
        "\t# Should be equal to the output of the MobileNetV2-SSD model.\n",
        "\tTOTAL_BOXES = sum([a*a*b for a,b in zip(layerWidths, numBoxes)])\t\t# Computes total number of boxes.\n",
        "\n",
        "\tcentres = np.zeros((TOTAL_BOXES,2))\n",
        "\thw = np.zeros((TOTAL_BOXES,2))\n",
        "\tboxes = np.zeros((TOTAL_BOXES,4))\n",
        "\n",
        "\t# Calculating the default boxes (centres, height, width)\n",
        "\tidx = 0\n",
        "\tfor grid_size, num_box, scale in zip(layerWidths, numBoxes, scales):\n",
        "\t\tstep_size = IMG_SIZE*1.0/grid_size\n",
        "\t\tfor i in range(grid_size):\n",
        "\t\t\tfor j in range(grid_size):\n",
        "\t\t\t\tpos = idx + (i*grid_size+j) * num_box\n",
        "\t\t\t\t# centre is the same for all aspect ratios(=num_box)\n",
        "\t\t\t\tcentres[ pos : pos + num_box , :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
        "\t\t\t\t# height and width vary according to the scale and aspect ratio\n",
        "\t\t\t\thw[ pos : pos + num_box , :] = np.multiply(grid_size*scale, np.squeeze(np.dstack([asp1, asp2]),axis=0))[:num_box,:]\n",
        "\n",
        "\t\tidx += grid_size*grid_size*num_box\n",
        "\n",
        "\n",
        "\t# (x,y) co-ordinates of top left and bottom right\n",
        "\t# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
        "\tboxes[:,0] = centres[:,0] - hw[:,0]/2\n",
        "\tboxes[:,1] = centres[:,1] - hw[:,1]/2\n",
        "\tboxes[:,2] = centres[:,0] + hw[:,0]/2\n",
        "\tboxes[:,3] = centres[:,1] + hw[:,1]/2\n",
        "\n",
        "\treturn boxes, TOTAL_BOXES, centres, hw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLwmN3MX16x"
      },
      "source": [
        "# calculate IoU for a set of search boxes and default boxes\n",
        "def IoU(box1, box2):\n",
        "\tbox1 = box1.astype(np.float64)\n",
        "\tbox2 = box2.astype(np.float64)\n",
        "\n",
        "\tx_top_left = np.maximum(box1[:,0], box2[:,0])\t\t\t# find x-coordinate of top-left corner for intersection.\n",
        "\tx_bottom_right = np.minimum(box1[:,2], box2[:,2])\t\t\t# find x-cordinate of bottom-right corner for intersection.\n",
        "\ty_top_left = np.maximum(box1[:,1], box2[:,1])\t\t\t# find y-coordinate of top-left corner for intersection.\n",
        "\ty_bottom_right = np.minimum(box1[:,3], box2[:,3])\t\t\t# find y-coordinate of bottom-right corner for intersection.\n",
        "\n",
        "\tintersection = np.abs(np.maximum(x_bottom_right - x_top_left,0) * np.maximum(y_bottom_right - y_top_left,0))\n",
        "\t\n",
        "\tboxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
        "\tboxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
        "\t\n",
        "\tunionArea = boxArea1 + boxArea2 - intersection\n",
        "\tassert (unionArea > 0).all()\n",
        "\treturn intersection / unionArea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hri8meClX3yv"
      },
      "source": [
        "# give the index of the box correpsonding to the IoUs > threshold (=0.5) \n",
        "def bestIoU(searchBox):\n",
        "\treturn np.argwhere(IoU(matlib.repmat(searchBox, TOTAL_BOXES, 1), boxes) > 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjgzps5zX5nz"
      },
      "source": [
        "boxes, TOTAL_BOXES, centres, hw = create_default_boxes()\n",
        "training_data_size = 2000\n",
        "testing_data_size = 100\n",
        "\n",
        "x_train = x_train[:training_data_size , : , :]\n",
        "y_train = y_train[:training_data_size]\n",
        "x_test = x_test[:testing_data_size , : , :]\n",
        "y_test = y_test[:testing_data_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG_ljuMUX7tE"
      },
      "source": [
        "# take mnist x and y pairs and convert to input, output pairs for the MobileNetv2+SSD model\n",
        "def create_dataset(images, labels, num_digits=3):\n",
        "\tMNIST_SIZE = images.shape[-1]\n",
        "\tscale_range = [1.5, 1, 2]\n",
        "\tcorners = np.array([np.random.randint(IMG_SIZE - int(MNIST_SIZE*max(scale_range)), size=(images.shape[0], 2)) for _ in range(num_digits)])\n",
        "\tdigits = np.array([np.random.randint(images.shape[0]) for _ in range(num_digits) for _ in range(images.shape[0])])\n",
        "\tdigits = digits.reshape(images.shape[0], num_digits)\n",
        "\tscales = np.array([np.random.choice(scale_range) for _ in range(num_digits) for _ in range(images.shape[0])])\n",
        "\tscales = scales.reshape(images.shape[0], num_digits)\n",
        "\n",
        "\t# Create a input image data.\n",
        "\tinput = np.zeros((images.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "\t# Add mnist digits in the images.\n",
        "\tfor idx in range(images.shape[0]):\n",
        "\t\tfor i in range(num_digits):\n",
        "\t\t\tSIZE = int(MNIST_SIZE*scales[idx, i])\n",
        "\t\t\tlx = corners[i, idx, 0]\n",
        "\t\t\tly = corners[i, idx, 1]\n",
        "\t\t\tinsertion_image = (resize(images[digits[idx, i],:,:], (SIZE, SIZE))*255).astype(np.uint8)\n",
        "\t\t\t# insertion_image = images[digits[idx, i],:,:]\n",
        "\t\t\tinput[idx, lx:lx+SIZE, ly:ly+SIZE, :] = np.repeat(np.expand_dims(np.array(insertion_image), axis=-1), 3, axis=-1)\n",
        "\n",
        "\t# Define the ground truth bounding boxes for each digit's image.\n",
        "    output = np.zeros((labels.shape[0], TOTAL_BOXES, 1+4))\t# [class + (cx, cy, h, w)] for each box.\n",
        "\toutput[:,:,0] = NUM_CLASSES\n",
        "\tfor idx in range(images.shape[0]):\n",
        "\t\tfor i in range(num_digits):\n",
        "\t\t\tSIZE = int(MNIST_SIZE*scales[idx, i])\n",
        "\t\t\tbbox = np.zeros(4)\n",
        "\t\t\tbbox[:2] = corners[i, idx]\n",
        "\t\t\tbbox[2:] = corners[i, idx] + (SIZE, SIZE)\n",
        "\t\t\tbox_idx = bestIoU(bbox).astype(np.uint16)\n",
        "\t\t\toutput[idx, box_idx, 0] = labels[digits[idx, i]]\n",
        "\t\t\toutput[idx, box_idx, 1] = (bbox[0] + bbox[2])/2.0 - centres[box_idx, 0]         # cx (difference between ground truth's center and default bounding box's center.)\n",
        "\t\t\toutput[idx, box_idx, 2] = (bbox[1] + bbox[3])/2.0 - centres[box_idx,1]          # cy\n",
        "\t\t\toutput[idx, box_idx, 3] = SIZE - hw[box_idx,0]                                  # delta h (difference between ground truth's size and default bounding box's size.)\n",
        "\t\t\toutput[idx, box_idx, 4] = SIZE - hw[box_idx,1]                                  # delta w\n",
        "\n",
        "\treturn input, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lfa5TnoX-B8",
        "outputId": "9a9ed241-62e8-4a85-d560-244612e1c739"
      },
      "source": [
        "test_x, test_y = create_dataset(x_test, y_test)\n",
        "train_x, train_y = create_dataset(x_train, y_train)\n",
        "\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "\n",
        "def _bytes_feature(value):\n",
        "\treturn tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def write_dataset(x_data, y_data, filename):\n",
        "\twriter = tf.compat.v1.python_io.TFRecordWriter(tfrecord_filename)\n",
        "\tfor x, y in zip(x_data, y_data):\n",
        "\t\ty = y.reshape(-1)\n",
        "\t\tx = x.reshape(-1)\n",
        "\t\tfeature = {'label': _bytes_feature(tf.compat.as_bytes(y.tostring())),\n",
        "\t\t\t\t   'image': _bytes_feature(tf.compat.as_bytes(x.tostring()))}\n",
        "\t\texample = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\t\twriter.write(example.SerializeToString())\n",
        "\n",
        "\twriter.close()\n",
        "\n",
        "tfrecord_filename = 'mnist_obj_detection_2000_train.tfrecords'\n",
        "write_dataset(train_x, train_y, tfrecord_filename)\n",
        "tfrecord_filename = 'mnist_obj_detection_100_test.tfrecords'\n",
        "write_dataset(test_x, test_y, tfrecord_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 224, 224, 3) (2000, 3150, 5)\n",
            "(100, 224, 224, 3) (100, 3150, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}