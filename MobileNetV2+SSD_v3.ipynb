{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2+SSD_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23wuE1X4ZbLe",
        "outputId": "7ba3c3d1-3f20-4b95-cd87-d377e255d674"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVv9a9uMZhj5",
        "outputId": "ef651a6a-502f-4d86-c0e1-144b57ac436b"
      },
      "source": [
        "ls"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CreateDataset.ipynb    MobileNetV2+SSD_v1.ipynb  TestKeras.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                  MobileNetV2+SSD_v2.ipynb\n",
            "MobileNetV2+SSD.ipynb  MobileNetV2+SSD_v3.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXuQP38aFi94"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy import matlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jL765qOfwkR"
      },
      "source": [
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "layerWidths = [28,14,7,4,2,1]\n",
        "numBoxes = [3,3,3,3,3,3]\n",
        "assert len(numBoxes) == len(layerWidths) # numBoxes for each layer and each layer has a specific width\n",
        "outputChannels = NUM_CLASSES + 1 + 4 # 10 classes + background + cx,cy,h,w\n",
        "assert outputChannels - NUM_CLASSES == 5"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8NPapjnWywx"
      },
      "source": [
        "# number of scales is equal to the number of different resolutions ie num of layer widths\n",
        "# for a given resolution, we have different aspect ratios\n",
        "MinScale = .1 # Min and Max scale given as percentage\n",
        "MaxScale = 1.5\n",
        "scales = [MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths))]\n",
        "scales = scales[::-1] # reversing the order because the layerWidths go from high to low (lower to higher resoltuion)\n",
        "\n",
        "asp = [0.5,1.0,1.5]\n",
        "asp1 = [x**0.5 for x in asp]\n",
        "asp2 = [1/x for x in asp1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fItMM4I5gD57",
        "outputId": "c76855ac-9276-43a8-bb81-7973411e48ae"
      },
      "source": [
        "# should be equal to the 1st dimension in the output layer of the SSD model\n",
        "BOXES = sum([a*a*b for a,b in zip(layerWidths,numBoxes)])\n",
        "centres = np.zeros((BOXES,2))\n",
        "hw = np.zeros((BOXES,2))\n",
        "boxes = np.zeros((BOXES,4))\n",
        "print(BOXES)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK0OkZhUgIJD"
      },
      "source": [
        "# calculating the default box centres and height, width\n",
        "idx = 0\n",
        "\n",
        "for gridSize, numBox, scale in zip(layerWidths,numBoxes,scales):\n",
        "    step_size = IMG_SIZE*1.0/gridSize\n",
        "    for i in range(gridSize):\n",
        "        for j in range(gridSize):\n",
        "            pos = idx + (i*gridSize+j) * numBox\n",
        "            # centre is the same for all bounding boxes\n",
        "            centres[pos:pos+numBox, :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
        "            # height and width vary according to the scale and aspect ratio\n",
        "            # zip asepct ratios and then scale them by the scaling factor\n",
        "            hw[pos:pos+numBox, :] = np.multiply(gridSize*scale, np.squeeze(np.dstack([asp1,asp2]),axis=0))[:numBox,:]\n",
        "\n",
        "    idx += gridSize*gridSize*numBox"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSzu-ma6gK-x"
      },
      "source": [
        "# (x,y) co-ordinates of top left and bottom right\n",
        "# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
        "boxes[:,0] = centres[:,0] - hw[:,0]/2\n",
        "boxes[:,1] = centres[:,1] - hw[:,1]/2\n",
        "boxes[:,2] = centres[:,0] + hw[:,0]/2\n",
        "boxes[:,3] = centres[:,1] + hw[:,1]/2"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLSYWbvh2kJ"
      },
      "source": [
        "# calculate IoU for a set of search boxes and default boxes\n",
        "def IoU(box1, box2):\n",
        "\tbox1 = box1.astype(np.float64)\n",
        "\tbox2 = box2.astype(np.float64)\n",
        "\n",
        "\tx_top_left = np.maximum(box1[:,0], box2[:,0])\t\t\t# find x-coordinate of top-left corner for intersection.\n",
        "\tx_bottom_right = np.minimum(box1[:,2], box2[:,2])\t\t\t# find x-cordinate of bottom-right corner for intersection.\n",
        "\ty_top_left = np.maximum(box1[:,1], box2[:,1])\t\t\t# find y-coordinate of top-left corner for intersection.\n",
        "\ty_bottom_right = np.minimum(box1[:,3], box2[:,3])\t\t\t# find y-coordinate of bottom-right corner for intersection.\n",
        "\n",
        "\tintersection = np.abs(np.maximum(x_bottom_right - x_top_left,0) * np.maximum(y_bottom_right - y_top_left,0))\n",
        "\t\n",
        "\tboxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
        "\tboxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
        "\t\n",
        "\tunionArea = boxArea1 + boxArea2 - intersection\n",
        "\tassert (unionArea > 0).all()\n",
        "\treturn intersection / unionArea"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z10Q5use7to"
      },
      "source": [
        "# give the index of the box correpsonding to the IoUs > threshold (=0.5) \n",
        "def bestIoU(searchBox):\n",
        "    return np.argwhere(IoU(matlib.repmat(searchBox,BOXES,1), boxes) > 0.5)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXwFqot5PPse"
      },
      "source": [
        "layerWidth = [28, 14, 7, 4, 2, 1]   # width of each bounding box.\n",
        "numBoxes = [3, 3, 3, 3, 3, 3]       # number of bound boxes for each width\n",
        "classes = 15    # 11 classes + 4 locations\n",
        "base_model=tf.keras.applications.MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)        # Use of MobileNetV2 as a backbone for SSD.\n",
        "\n",
        "feature0 = base_model.get_layer('block_6_expand_relu').output\n",
        "feature1 = base_model.get_layer('block_13_expand_relu').output\n",
        "\n",
        "featureMaps = 6\n",
        "features = [None for _ in range(featureMaps)]\n",
        "classifiers = [None for _ in range(featureMaps)]\n",
        "\n",
        "conv1_1 = tf.keras.layers.Conv2D(256,1,name='SSD_conv_1_1')\n",
        "conv1_2 = tf.keras.layers.Conv2D(512,3,strides=(2,2),padding='same',name='SSD_conv_1_2')\n",
        "\n",
        "conv2_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_2_1')\n",
        "conv2_2 = tf.keras.layers.Conv2D(256,3,strides=(2,2),padding='same',name='SSD_conv_2_2')\n",
        "\n",
        "conv3_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_3_1')\n",
        "conv3_2 = tf.keras.layers.Conv2D(256,3,strides=(1,1),name='SSD_conv_3_2')\n",
        "\n",
        "conv4_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_4_1')\n",
        "conv4_2 = tf.keras.layers.Conv2D(256,2,strides=(1,1),name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n",
        "\n",
        "conv = []\n",
        "reshape = []\n",
        "\n",
        "for i in range(featureMaps):\n",
        "    conv.append(tf.keras.layers.Conv2D(numBoxes[i]*classes, 3, padding='same', name='Classification_'+str(i)))\n",
        "    reshape.append(tf.keras.layers.Reshape((layerWidth[i]* layerWidth[i] * numBoxes[i], classes),name='Reshape_classification_'+str(i)))\n",
        "\n",
        "# Use of different spatial features.\n",
        "features[0] = feature0\n",
        "features[1] = feature1\n",
        "features[2] = conv1_2(conv1_1(features[1]))\n",
        "features[3] = conv2_2(conv2_1(features[2]))\n",
        "features[4] = conv3_2(conv3_1(features[3]))\n",
        "features[5] = conv4_2(conv4_1(features[4]))\n",
        "\n",
        "for i in range(featureMaps):\n",
        "    x = conv[i](features[i])            # Apply 1x1 convolutions to generate bounding boxes.\n",
        "    x = reshape[i](x)                   # Reshape the output to (batch_size x TOTAL_BOXES x classes)\n",
        "    classifiers[i] = x\n",
        "output = tf.keras.layers.concatenate(classifiers, axis = -2, name='concatenate')\n",
        "\n",
        "model=tf.keras.Model(inputs=base_model.input,outputs=output) #specify the inputs and outputs\n",
        "# model.summary()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDaTtWagzW6B"
      },
      "source": [
        "Following functions are used to access the dataset from tfrecords. (Used https://dzlab.github.io/dltips/en/tensorflow/tfrecord/ as a reference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiTFiYyRFNM"
      },
      "source": [
        "def _parse_function(proto):\n",
        "\t# define your tfrecord again. Remember that you saved your image as a string.\n",
        "\tkeys_to_features = {'image': tf.io.FixedLenFeature([], tf.string),\n",
        "\t\t\t\t\t\t'label': tf.io.FixedLenFeature([], tf.string)}\n",
        "\t\n",
        "\t# Load one example\n",
        "\tparsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
        "\t\n",
        "\t# Turn your saved image string into an array\n",
        "\tparsed_features['image'] = tf.io.decode_raw(parsed_features['image'], tf.float64)\n",
        "\tparsed_features['image'] = tf.reshape(parsed_features['image'], (224, 224, 3))\n",
        "\tparsed_features['label'] = tf.io.decode_raw(parsed_features['label'], tf.float64)\n",
        "\tparsed_features['label'] = tf.reshape(parsed_features['label'], (3150, 5))\n",
        "\n",
        "\treturn parsed_features['image'], parsed_features['label']\n",
        "\n",
        "  \n",
        "def create_dataset(filepath):\n",
        "\t\n",
        "\t# This works with arrays as well\n",
        "\tdataset = tf.data.TFRecordDataset(filepath)\n",
        "\t\n",
        "\t# Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
        "\tdataset = dataset.map(_parse_function, num_parallel_calls=8)\n",
        "\treturn dataset"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6mXt7Y7Y9tu"
      },
      "source": [
        "train_dataset = create_dataset(['data/mnist_obj_detection_2000_train.tfrecords'])\n",
        "test_dataset = create_dataset(['data/mnist_obj_detection_100_test.tfrecords'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM2jG3BoRJKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13e1462-262d-4efa-f6ea-20934e83587d"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "SHUFFLE_BUFFER_SIZE = 60\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((10, 224, 224, 3), (10, 3150, 5)), types: (tf.float64, tf.float64)>\n",
            "<BatchDataset shapes: ((10, 224, 224, 3), (10, 3150, 5)), types: (tf.float64, tf.float64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOV-HjtxyhU-"
      },
      "source": [
        "Smooth L1 loss function's code is based on the function given at https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDHTUkF6RMLb"
      },
      "source": [
        "# calculate the smooth L1 loss\n",
        "def smoothL1(x,y):\n",
        "    diff = K.abs(x-y)\n",
        "    result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
        "    return K.mean(result)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwBzQGiAwty0"
      },
      "source": [
        "Below function for hard negative mining is referred from this repository\n",
        "https://github.com/ChunML/ssd-tf2/blob/master/losses.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4fcyHF-vCQr"
      },
      "source": [
        "def hard_negative_mining(loss, gt_confs, neg_ratio=3):\n",
        "    pos_idx = gt_confs > 9\n",
        "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
        "    num_neg = num_pos * neg_ratio\n",
        "\n",
        "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
        "    rank = tf.argsort(rank, axis=1)\n",
        "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
        "\n",
        "    return pos_idx, neg_idx"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqm7OoPtRUOc"
      },
      "source": [
        "def confidenceLoss(y, label):\n",
        "    classification_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
        "    pos_idx, neg_idx = hard_negative_mining(classification_loss, label)\n",
        "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    classification_loss = cross_entropy(label[tf.math.logical_or(pos_idx, neg_idx)], y[tf.math.logical_or(pos_idx, neg_idx)])\n",
        "    return K.mean(classification_loss)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byz853l8RayD"
      },
      "source": [
        "def Loss(gt, y):\n",
        "    # shape of y is batch_size * BOXES * output_channels\n",
        "    # shape of gt is batch_size * BOXES * 5 \n",
        "    loss = 0\n",
        "    # localisation loss\n",
        "    loss += smoothL1(y[:,:,-4:], gt[:,:,-4:])\n",
        "    # confidence loss\n",
        "    loss += confidenceLoss(y[:,:,:-4], tf.cast(gt[:,:,0],tf.int32))\n",
        "    return loss"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU4XVmr2Rg4Z"
      },
      "source": [
        "base_learning_rate = 0.00001\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),loss=Loss)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJmRwVCRkYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c250e665-7b5f-4c5d-fce2-247136a4d5ae"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=15, validation_data = test_dataset)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "200/200 [==============================] - 26s 99ms/step - loss: 0.0912 - val_loss: 0.3651\n",
            "Epoch 2/15\n",
            "200/200 [==============================] - 20s 96ms/step - loss: 0.0904 - val_loss: 0.3046\n",
            "Epoch 3/15\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0900 - val_loss: 0.2575\n",
            "Epoch 4/15\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0899 - val_loss: 0.2417\n",
            "Epoch 5/15\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0892 - val_loss: 0.2351\n",
            "Epoch 6/15\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0888 - val_loss: 0.1702\n",
            "Epoch 7/15\n",
            "200/200 [==============================] - 20s 96ms/step - loss: 0.0887 - val_loss: 0.1082\n",
            "Epoch 8/15\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0883 - val_loss: 0.1031\n",
            "Epoch 9/15\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0882 - val_loss: 0.1009\n",
            "Epoch 10/15\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0879 - val_loss: 0.1001\n",
            "Epoch 11/15\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.0875 - val_loss: 0.1001\n",
            "Epoch 12/15\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.0870 - val_loss: 0.0966\n",
            "Epoch 13/15\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0871 - val_loss: 0.0965\n",
            "Epoch 14/15\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0869 - val_loss: 0.0965\n",
            "Epoch 15/15\n",
            "200/200 [==============================] - 19s 95ms/step - loss: 0.0864 - val_loss: 0.0953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYujGKBxM47V",
        "outputId": "078049e3-da95-4faf-8431-560c4c698625"
      },
      "source": [
        "# get prediction for one sample\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(test_dataset)\n",
        "image, label = iterator.next()\n",
        "y_pred = model.predict(image)\n",
        "y_pred.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3150, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmnRvhMhyGXj"
      },
      "source": [
        "Below visulization codes are referred from other repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBMo0v3VM9Xr"
      },
      "source": [
        "import bottleneck\n",
        "OBJperCLASS = 10 # get the top 10 results for each class\n",
        "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
        "def infer(Y):\n",
        "    # classes are actually the index into the default boxes\n",
        "    classes = np.zeros((OBJperCLASS, outputChannels-4), dtype=np.uint16)\n",
        "    conf = np.zeros((OBJperCLASS, outputChannels-4))\n",
        "    delta = np.zeros((OBJperCLASS, outputChannels-4,4))\n",
        "    class_predictions = softmax(Y[:,:outputChannels-4],axis=1)\n",
        "    for i in range(outputChannels-4):\n",
        "        classes[:,i] = bottleneck.argpartition(class_predictions[:,i],BOXES-1-10,axis=-1)[-OBJperCLASS:]\n",
        "        conf[:,i] = class_predictions[classes[:,i],i]\n",
        "        delta[:,i] = Y[classes[:,i],outputChannels-4:]\n",
        "    return conf,classes, delta\n",
        "\n",
        "# generate bounding boxes from the inferred outputs\n",
        "def Bbox(confidence,box_idx,delta):\n",
        "    #delta contains delta(cx,cy,h,w)\n",
        "    bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
        "    bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
        "    for i in range(OBJperCLASS):\n",
        "        bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
        "        bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
        "        bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
        "        bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n",
        "    return bbox_centre,bbox_hw"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "KeclD8VkNAJr",
        "outputId": "c1e51fbf-3fd5-470d-d430-87f564610722"
      },
      "source": [
        "idx = np.random.randint(BATCH_SIZE)\n",
        "from scipy.special import softmax\n",
        "\n",
        "# top 10 predictions for each class\n",
        "confidence, box_idx, delta = infer(y_pred[idx])\n",
        "bbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n",
        "\n",
        "im = np.array(Image.fromarray(image[idx].numpy().astype(np.uint8)))\n",
        "fig,ax = plt.subplots(1)\n",
        "ax.imshow(im)\n",
        "\n",
        "for i in range(outputChannels-4):\n",
        "  # skipping backgrounds\n",
        "    if i == NUM_CLASSES:\n",
        "        continue\n",
        "    color = 'r'\n",
        "    # if a class is mentioned in the ground truth, color the boxes green\n",
        "    if i in label[idx,:,0]:\n",
        "        color = 'g'\n",
        "        print(i)\n",
        "    \n",
        "    # skip all the classes which have low confidence values\n",
        "    if (confidence[:,i] > 0.5).any() or i in label[idx,:,0]:\n",
        "        for k in range(OBJperCLASS):\n",
        "            print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n",
        "      \n",
        "            # draw bounding box only if confidence scores are high\n",
        "            if confidence[k,i] < 0.5:\n",
        "                continue\n",
        "            x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n",
        "            y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n",
        "            rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1: Confidence-0.9503666758537292\t\tCentre-[139.75433174 184.03794456] Height,Width-[47.4474152  39.30059328]\n",
            "1: Confidence-0.9957889318466187\t\tCentre-[138.46671438 176.01256466] Height,Width-[51.66868195 45.55496015]\n",
            "1: Confidence-0.9973377585411072\t\tCentre-[137.60649538 177.32022214] Height,Width-[49.41812083 49.38247344]\n",
            "1: Confidence-0.9954240322113037\t\tCentre-[138.61615217 175.60827994] Height,Width-[50.0746741 49.0065932]\n",
            "1: Confidence-0.995050311088562\t\tCentre-[138.78298044 177.70707321] Height,Width-[51.5087069 46.0172595]\n",
            "1: Confidence-0.9833537340164185\t\tCentre-[135.98721528 175.02206922] Height,Width-[46.35291812 46.65254065]\n",
            "1: Confidence-0.843590497970581\t\tCentre-[146.42103171 179.46820474] Height,Width-[41.88904473 40.80016514]\n",
            "1: Confidence-0.8024263381958008\t\tCentre-[133.67000008 174.51735139] Height,Width-[49.37204918 39.34166517]\n",
            "1: Confidence-0.9384608268737793\t\tCentre-[133.67521024 176.78495622] Height,Width-[48.69663367 40.32307424]\n",
            "1: Confidence-0.9780067801475525\t\tCentre-[136.71280384 176.98350692] Height,Width-[46.62253806 46.7173619 ]\n",
            "4\n",
            "4: Confidence-0.9579575657844543\t\tCentre-[ 40.37991428 180.57222176] Height,Width-[43.41936008 42.64436712]\n",
            "4: Confidence-0.985541045665741\t\tCentre-[ 38.25947332 181.83359182] Height,Width-[45.04188487 44.23796317]\n",
            "4: Confidence-0.9581587314605713\t\tCentre-[ 39.19197845 180.06973982] Height,Width-[43.51976947 44.60255517]\n",
            "4: Confidence-0.9600951671600342\t\tCentre-[ 39.13716888 181.34196174] Height,Width-[43.19959864 42.24211683]\n",
            "4: Confidence-0.962906539440155\t\tCentre-[ 40.14146042 179.85790634] Height,Width-[44.11490771 44.84231898]\n",
            "4: Confidence-0.9811779260635376\t\tCentre-[ 40.71430016 181.67912638] Height,Width-[44.26297745 41.68964567]\n",
            "4: Confidence-0.9588091969490051\t\tCentre-[ 38.76185799 179.91623878] Height,Width-[44.07220503 44.13650271]\n",
            "4: Confidence-0.9921649694442749\t\tCentre-[ 40.53808975 181.17740929] Height,Width-[43.4301705  43.38189885]\n",
            "4: Confidence-0.9881364703178406\t\tCentre-[ 40.29069662 181.0191766 ] Height,Width-[42.72398227 45.81708419]\n",
            "4: Confidence-0.9721844792366028\t\tCentre-[ 37.98296499 181.41436303] Height,Width-[44.34986537 44.17012726]\n",
            "5\n",
            "5: Confidence-0.8695726990699768\t\tCentre-[131.21338105  51.17150784] Height,Width-[50.27308163 44.53700819]\n",
            "5: Confidence-0.8625978827476501\t\tCentre-[130.94788337  52.0966809 ] Height,Width-[43.86218303 54.55194556]\n",
            "5: Confidence-0.9915905594825745\t\tCentre-[130.12097895  53.34157789] Height,Width-[56.78328273 56.49165675]\n",
            "5: Confidence-0.97519451379776\t\tCentre-[130.6139586   53.70932174] Height,Width-[55.68428978 53.41109647]\n",
            "5: Confidence-0.9238817095756531\t\tCentre-[128.91740942  54.7764504 ] Height,Width-[55.69439074 55.21072718]\n",
            "5: Confidence-0.9792057871818542\t\tCentre-[130.66737425  54.36862803] Height,Width-[53.57125839 51.80262556]\n",
            "5: Confidence-0.8767765760421753\t\tCentre-[125.90265322  54.91152   ] Height,Width-[54.04167923 51.34773435]\n",
            "5: Confidence-0.9912745356559753\t\tCentre-[130.43677163  53.73762417] Height,Width-[52.86143824 53.09065386]\n",
            "5: Confidence-0.9729942679405212\t\tCentre-[127.8737545  54.2826128] Height,Width-[53.19273516 53.2245135 ]\n",
            "5: Confidence-0.9015135765075684\t\tCentre-[126.88187313  55.32193232] Height,Width-[52.93390831 48.66805639]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}