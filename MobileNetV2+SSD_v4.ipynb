{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2+SSD_v4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23wuE1X4ZbLe",
        "outputId": "0a62dbbc-f51b-4a7d-8597-841ce59f35c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVv9a9uMZhj5",
        "outputId": "59317dcc-ee95-497f-9258-26678d891497"
      },
      "source": [
        "ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CreateDataset.ipynb    MobileNetV2+SSD_v1.ipynb  MobileNetV2+SSD_v4.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                  MobileNetV2+SSD_v2.ipynb  TestKeras.ipynb\n",
            "MobileNetV2+SSD.ipynb  MobileNetV2+SSD_v3.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXuQP38aFi94"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy import matlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jL765qOfwkR"
      },
      "source": [
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "layerWidths = [28,14,7,4,2,1]\n",
        "numBoxes = [3,3,3,3,3,3]\n",
        "assert len(numBoxes) == len(layerWidths)        # numBoxes for each layer and each layer has a specific width\n",
        "outputChannels = NUM_CLASSES + 1 + 4            # 10 classes + background + cx,cy,h,w\n",
        "assert outputChannels - NUM_CLASSES == 5"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGtGIEkvQkFT"
      },
      "source": [
        "Reference is taken from this blog. https://d2l.ai/chapter_computer-vision/anchor.html \\\\\n",
        "\"*Assume that the input image has a height of h and width of w. We generate anchor boxes with different shapes centered on each pixel of the image. Assume the size is s∈(0,1], the aspect ratio is r>0, and the width and height of the anchor box are ws√r and hs/√r, respectively. When the center position is given, an anchor box with known width and height is determined.*\" \\\\\n",
        "\n",
        "s: scale, h: grid_size, w: grid_size and r: asp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrXPAnHPHas5"
      },
      "source": [
        "def create_default_boxes():\n",
        "\t# number of scales is equal to the number of different resolutions ie num of layer widths\n",
        "\t# for a given resolution, we have different aspect ratios\n",
        "\t# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\n",
        "\tMinScale = .1 \t\t\t\t\t# Min and Max scale given as percentage\n",
        "\tMaxScale = 1.5\n",
        "\tscales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths))]\n",
        "\tscales = scales[::-1] \t\t\t\t\t\t# reversing the order because the layer_widths go from high to low (lower to higher resoltuion)\n",
        "\n",
        "\tasp = [0.5,1.0,1.5]\n",
        "\tasp1 = [x**0.5 for x in asp]\n",
        "\tasp2 = [1/x for x in asp1]\n",
        "\n",
        "\t# Should be equal to the output of the MobileNetV2-SSD model.\n",
        "\tTOTAL_BOXES = sum([a*a*b for a,b in zip(layerWidths, numBoxes)])\t\t# Computes total number of boxes. (Sum of (layer_widths*layer_widths*num_boxes))\n",
        "\n",
        "\tcentres = np.zeros((TOTAL_BOXES,2))\n",
        "\thw = np.zeros((TOTAL_BOXES,2))\n",
        "\tboxes = np.zeros((TOTAL_BOXES,4))\n",
        "\n",
        "\t# Calculating the default boxes (centres, height, width)\n",
        "\tidx = 0\n",
        "\tfor grid_size, num_box, scale in zip(layerWidths, numBoxes, scales):\n",
        "\t\tstep_size = IMG_SIZE*1.0/grid_size\n",
        "\t\tfor i in range(grid_size):\n",
        "\t\t\tfor j in range(grid_size):\n",
        "\t\t\t\tpos = idx + (i*grid_size+j) * num_box\n",
        "\t\t\t\t# centre is the same for all aspect ratios(=num_box)\n",
        "\t\t\t\tcentres[ pos : pos + num_box , :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
        "\t\t\t\t# height and width vary according to the scale and aspect ratio\n",
        "\t\t\t\thw[ pos : pos + num_box , :] = np.multiply(grid_size*scale, np.squeeze(np.dstack([asp1, asp2]),axis=0))[:num_box,:]\n",
        "\n",
        "\t\tidx += grid_size*grid_size*num_box\n",
        "\n",
        "\n",
        "\t# (x,y) co-ordinates of top left and bottom right\n",
        "\t# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
        "\tboxes[:,0] = centres[:,0] - hw[:,0]/2\n",
        "\tboxes[:,1] = centres[:,1] - hw[:,1]/2\n",
        "\tboxes[:,2] = centres[:,0] + hw[:,0]/2\n",
        "\tboxes[:,3] = centres[:,1] + hw[:,1]/2\n",
        "\n",
        "\treturn boxes, TOTAL_BOXES, centres, hw"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSzu-ma6gK-x"
      },
      "source": [
        "boxes, TOTAL_BOXES, centres, hw = create_default_boxes()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLSYWbvh2kJ"
      },
      "source": [
        "# calculate IoU for a set of search boxes and default boxes\n",
        "def IoU(box1, box2):\n",
        "\tbox1 = box1.astype(np.float64)\n",
        "\tbox2 = box2.astype(np.float64)\n",
        "\n",
        "\tx_top_left = np.maximum(box1[:,0], box2[:,0])\t\t\t# find x-coordinate of top-left corner for intersection.\n",
        "\tx_bottom_right = np.minimum(box1[:,2], box2[:,2])\t\t\t# find x-cordinate of bottom-right corner for intersection.\n",
        "\ty_top_left = np.maximum(box1[:,1], box2[:,1])\t\t\t# find y-coordinate of top-left corner for intersection.\n",
        "\ty_bottom_right = np.minimum(box1[:,3], box2[:,3])\t\t\t# find y-coordinate of bottom-right corner for intersection.\n",
        "\n",
        "\tintersection = np.abs(np.maximum(x_bottom_right - x_top_left,0) * np.maximum(y_bottom_right - y_top_left,0))\n",
        "\t\n",
        "\tboxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
        "\tboxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
        "\t\n",
        "\tunionArea = boxArea1 + boxArea2 - intersection\n",
        "\tassert (unionArea > 0).all()\n",
        "\treturn intersection / unionArea"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z10Q5use7to"
      },
      "source": [
        "# give the index of the box correpsonding to the IoUs > threshold (=0.5) \n",
        "def bestIoU(searchBox):\n",
        "    return np.argwhere(IoU(matlib.repmat(searchBox, TOTAL_BOXES, 1), boxes) > 0.5)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXwFqot5PPse"
      },
      "source": [
        "classes = 15    # 10 classes + 1 background + 4 locations\n",
        "base_model=tf.keras.applications.MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)        # Use of MobileNetV2 as a backbone for SSD.\n",
        "\n",
        "feature0 = base_model.get_layer('block_6_expand_relu').output\n",
        "feature1 = base_model.get_layer('block_13_expand_relu').output\n",
        "\n",
        "featureMaps = 6\n",
        "features = [None for _ in range(featureMaps)]\n",
        "classifiers = [None for _ in range(featureMaps)]\n",
        "\n",
        "conv1_1 = tf.keras.layers.Conv2D(256,1,name='SSD_conv_1_1')\n",
        "conv1_2 = tf.keras.layers.Conv2D(512,3,strides=(2,2),padding='same',name='SSD_conv_1_2')\n",
        "\n",
        "conv2_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_2_1')\n",
        "conv2_2 = tf.keras.layers.Conv2D(256,3,strides=(2,2),padding='same',name='SSD_conv_2_2')\n",
        "\n",
        "conv3_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_3_1')\n",
        "conv3_2 = tf.keras.layers.Conv2D(256,3,strides=(1,1),name='SSD_conv_3_2')\n",
        "\n",
        "conv4_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_4_1')\n",
        "conv4_2 = tf.keras.layers.Conv2D(256,2,strides=(1,1),name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n",
        "\n",
        "conv = []\n",
        "reshape = []\n",
        "\n",
        "for i in range(featureMaps):\n",
        "    conv.append(tf.keras.layers.Conv2D(numBoxes[i]*outputChannels, 3, padding='same', name='Classification_'+str(i)))\n",
        "    reshape.append(tf.keras.layers.Reshape((layerWidths[i]* layerWidths[i] * numBoxes[i], outputChannels),name='Reshape_classification_'+str(i)))\n",
        "\n",
        "# Use of different spatial features.\n",
        "features[0] = feature0\n",
        "features[1] = feature1\n",
        "features[2] = conv1_2(conv1_1(features[1]))\n",
        "features[3] = conv2_2(conv2_1(features[2]))\n",
        "features[4] = conv3_2(conv3_1(features[3]))\n",
        "features[5] = conv4_2(conv4_1(features[4]))\n",
        "\n",
        "for i in range(featureMaps):\n",
        "    x = conv[i](features[i])            # Apply 1x1 convolutions to generate bounding boxes.\n",
        "    x = reshape[i](x)                   # Reshape the output to (batch_size x TOTAL_BOXES x classes)\n",
        "    classifiers[i] = x\n",
        "output = tf.keras.layers.concatenate(classifiers, axis = -2, name='concatenate')\n",
        "\n",
        "model=tf.keras.Model(inputs=base_model.input,outputs=output) #specify the inputs and outputs\n",
        "# model.summary()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDaTtWagzW6B"
      },
      "source": [
        "Following functions are used to access the dataset from tfrecords. (Used https://dzlab.github.io/dltips/en/tensorflow/tfrecord/ as a reference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiTFiYyRFNM"
      },
      "source": [
        "def _parse_function(proto):\n",
        "\t# define your tfrecord again. Remember that you saved your image as a string.\n",
        "\tkeys_to_features = {'image': tf.io.FixedLenFeature([], tf.string),\n",
        "\t\t\t\t\t\t'label': tf.io.FixedLenFeature([], tf.string)}\n",
        "\t\n",
        "\t# Load one example\n",
        "\tparsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
        "\t\n",
        "\t# Turn your saved image string into an array\n",
        "\tparsed_features['image'] = tf.io.decode_raw(parsed_features['image'], tf.float64)\n",
        "\tparsed_features['image'] = tf.reshape(parsed_features['image'], (224, 224, 3))\n",
        "\tparsed_features['label'] = tf.io.decode_raw(parsed_features['label'], tf.float64)\n",
        "\tparsed_features['label'] = tf.reshape(parsed_features['label'], (3150, 5))\n",
        "\n",
        "\treturn parsed_features['image'], parsed_features['label']\n",
        "\n",
        "  \n",
        "def create_dataset(filepath):\n",
        "\t\n",
        "\t# This works with arrays as well\n",
        "\tdataset = tf.data.TFRecordDataset(filepath)\n",
        "\t\n",
        "\t# Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
        "\tdataset = dataset.map(_parse_function, num_parallel_calls=8)\n",
        "\treturn dataset"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6mXt7Y7Y9tu"
      },
      "source": [
        "train_dataset = create_dataset(['data/mnist_obj_detection_2000_train.tfrecords'])\n",
        "test_dataset = create_dataset(['data/mnist_obj_detection_100_test.tfrecords'])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM2jG3BoRJKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cab56b2-5de0-4ca8-8044-b7a5165518db"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "SHUFFLE_BUFFER_SIZE = 60\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((10, 224, 224, 3), (10, 3150, 5)), types: (tf.float64, tf.float64)>\n",
            "<BatchDataset shapes: ((10, 224, 224, 3), (10, 3150, 5)), types: (tf.float64, tf.float64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOV-HjtxyhU-"
      },
      "source": [
        "Smooth L1 loss function's code is based on the function given at https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDHTUkF6RMLb"
      },
      "source": [
        "# calculate the smooth L1 loss\n",
        "def smoothL1(x,y):\n",
        "    diff = K.abs(x-y)\n",
        "    result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
        "    return K.mean(result)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwBzQGiAwty0"
      },
      "source": [
        "Below function for hard negative mining is referred from this repository\n",
        "https://github.com/ChunML/ssd-tf2/blob/master/losses.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzEBHOsK7TLg"
      },
      "source": [
        "# This is the old code for negative hard mining.\n",
        "# def hard_negative_mining(loss, gt_confs, neg_ratio=3):\n",
        "#     pos_idx = gt_confs < 10\n",
        "#     num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
        "#     num_neg = num_pos * neg_ratio\n",
        "\n",
        "#     rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
        "#     rank = tf.argsort(rank, axis=1)\n",
        "#     neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
        "\n",
        "#     return pos_idx, neg_idx\n",
        "\n",
        "# def confidenceLoss(y, label):\n",
        "#     classification_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
        "#     pos_idx, neg_idx = hard_negative_mining(classification_loss, label)\n",
        "#     cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "#     classification_loss = cross_entropy(label[tf.math.logical_or(pos_idx, neg_idx)], y[tf.math.logical_or(pos_idx, neg_idx)])\n",
        "#     return K.mean(classification_loss)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crs0T3_O9Lc5"
      },
      "source": [
        "The following loss function is written based on the information given in this blog (https://becominghuman.ai/tensorflow-object-detection-api-basics-of-detection-7b134d689c75)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBgwXQ8dTUTp"
      },
      "source": [
        "# Contains implementation of Hard Negative Mining.\n",
        "def confidenceLoss(y, label):\n",
        "    total_classification_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
        "\n",
        "    positive_indices = label < 10                                           # Find boxes having mnist digits.\n",
        "    positive_classification_loss = tf.math.reduce_mean(total_classification_loss[positive_indices])\n",
        "\n",
        "    negative_indices = 1.0 - tf.cast(positive_indices, tf.float32)                                 # Find boxes having no mnist digits.\n",
        "    negative_classification_loss = total_classification_loss*negative_indices                      # Make loss of all postive boxes as zero.\n",
        "    negative_classification_loss = tf.sort(negative_classification_loss, axis=1, direction='DESCENDING')\n",
        "    \n",
        "    positive_indices = tf.cast(positive_indices, tf.int32)                  # Cast the boolean tensor to integers.\n",
        "    total_num_positives = tf.math.reduce_sum(positive_indices, axis=1)      # Find the total number of boxes containing mnist digits.\n",
        "    required_num_negatives = tf.cast(3*total_num_positives, tf.int32)       # Usual ratio of negative examples to positive examples is 3:1. (This will give value of k to choose top-k negative losses.)\n",
        "    \n",
        "    batch_size, number_images = negative_classification_loss.shape          # Find batch size and number of image.\n",
        "    array = tf.constant(np.array([np.arange(number_images) for _ in range(batch_size)]))        # Create an array to select top-k negative losses.\n",
        "    array = tf.cast(array, tf.int32)\n",
        "    negative_indices = array < tf.expand_dims(required_num_negatives, axis=1)                               # Make True for top-k elements and the rest to be False.\n",
        "    negative_classification_loss = tf.math.reduce_mean(negative_classification_loss[negative_indices])       # Compute negative loss using top-k elements.\n",
        "\n",
        "    return tf.math.reduce_mean([positive_classification_loss, negative_classification_loss])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byz853l8RayD"
      },
      "source": [
        "def Loss(gt, y):\n",
        "    # shape of y is batch_size * BOXES * outputChannels\n",
        "    # shape of gt is batch_size * BOXES * 5 \n",
        "    loss = 0\n",
        "    # localisation loss\n",
        "    loss += smoothL1(y[:,:,-4:], gt[:,:,-4:])\n",
        "    # confidence loss\n",
        "    loss += confidenceLoss(y[:,:,:-4], tf.cast(gt[:,:,0],tf.int32))\n",
        "    return loss"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU4XVmr2Rg4Z"
      },
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),loss=Loss)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJmRwVCRkYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928e5b7a-53c6-4194-b546-c0bae30d2db1"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=20, validation_data = test_dataset)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "200/200 [==============================] - 26s 97ms/step - loss: 2.8193 - val_loss: 1.5974\n",
            "Epoch 2/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 1.2201 - val_loss: 1.3494\n",
            "Epoch 3/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.9411 - val_loss: 1.2667\n",
            "Epoch 4/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.7933 - val_loss: 1.1250\n",
            "Epoch 5/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.7008 - val_loss: 0.9922\n",
            "Epoch 6/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.6398 - val_loss: 0.9441\n",
            "Epoch 7/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.5918 - val_loss: 0.9066\n",
            "Epoch 8/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.5527 - val_loss: 0.8110\n",
            "Epoch 9/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.5263 - val_loss: 0.7369\n",
            "Epoch 10/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.4979 - val_loss: 0.6946\n",
            "Epoch 11/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.4755 - val_loss: 0.6809\n",
            "Epoch 12/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.4576 - val_loss: 0.6383\n",
            "Epoch 13/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.4417 - val_loss: 0.6489\n",
            "Epoch 14/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.4241 - val_loss: 0.6014\n",
            "Epoch 15/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.4098 - val_loss: 0.6319\n",
            "Epoch 16/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.3943 - val_loss: 0.5952\n",
            "Epoch 17/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.3833 - val_loss: 0.5909\n",
            "Epoch 18/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.3718 - val_loss: 0.6046\n",
            "Epoch 19/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.3666 - val_loss: 0.5585\n",
            "Epoch 20/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.3535 - val_loss: 0.5313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYujGKBxM47V",
        "outputId": "e1dbe775-0e2c-470b-9d3d-25456ae00c24"
      },
      "source": [
        "# get prediction for one sample\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(test_dataset)\n",
        "image, label = iterator.next()\n",
        "y_pred = model.predict(image)\n",
        "y_pred.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3150, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmnRvhMhyGXj"
      },
      "source": [
        "Below visulization codes are referred from other repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBMo0v3VM9Xr"
      },
      "source": [
        "import bottleneck\n",
        "OBJperCLASS = 10 # get the top 10 results for each class\n",
        "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
        "def infer(Y):\n",
        "    # classes are actually the index into the default boxes\n",
        "    classes = np.zeros((OBJperCLASS, outputChannels-4), dtype=np.uint16)        # [10, 11]\n",
        "    conf = np.zeros((OBJperCLASS, outputChannels-4))                            # [10, 11]\n",
        "    delta = np.zeros((OBJperCLASS, outputChannels-4, 4))                        # [10, 11, 4]\n",
        "    class_predictions = softmax(Y[:,:outputChannels-4],axis=1)                  # [3150, 11]\n",
        "    for i in range(outputChannels-4):                                           # (Loop to find boxes for each mnist digit class)\n",
        "        # Basically the below line is used to find 11 boxes for the present mnist digit.\n",
        "        classes[:,i] = bottleneck.argpartition(class_predictions[:,i], TOTAL_BOXES-1-10 , axis=-1)[-OBJperCLASS:]      # It will take class_prediction[:, i] (sized 3150,1) and argpartition at the 3150-11th element from last.\n",
        "        conf[:,i] = class_predictions[classes[:,i],i]               # Confidence score for each digit.\n",
        "        delta[:,i] = Y[classes[:,i],outputChannels-4:]              # Used to find cx, cy, h, w.\n",
        "    return conf, classes, delta\n",
        "\n",
        "# generate bounding boxes from the inferred outputs\n",
        "def Bbox(confidence,box_idx,delta):\n",
        "    #delta contains delta(cx,cy,h,w)\n",
        "    bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
        "    bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
        "    for i in range(OBJperCLASS):\n",
        "        bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
        "        bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
        "        bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
        "        bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n",
        "    return bbox_centre,bbox_hw"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "KeclD8VkNAJr",
        "outputId": "91884977-ce08-45fb-f673-1f4b6bef9764"
      },
      "source": [
        "idx = np.random.randint(BATCH_SIZE)\n",
        "from scipy.special import softmax\n",
        "\n",
        "# top 10 predictions for each class\n",
        "confidence, box_idx, delta = infer(y_pred[idx])\n",
        "bbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n",
        "\n",
        "im = np.array(Image.fromarray(image[idx].numpy().astype(np.uint8)))\n",
        "fig,ax = plt.subplots(1)\n",
        "ax.imshow(im)\n",
        "\n",
        "for i in range(outputChannels-4):\n",
        "  # skipping backgrounds\n",
        "    if i == NUM_CLASSES:\n",
        "        continue\n",
        "    color = 'r'\n",
        "    # if a class is mentioned in the ground truth, color the boxes green\n",
        "    if i in label[idx,:,0]:\n",
        "        color = 'g'\n",
        "        print(i)\n",
        "    \n",
        "    # skip all the classes which have low confidence values\n",
        "    if (confidence[:,i] > 0.5).any() or i in label[idx,:,0]:\n",
        "        for k in range(OBJperCLASS):\n",
        "            print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n",
        "      \n",
        "            # draw bounding box only if confidence scores are high\n",
        "            if confidence[k,i] < 0.5:\n",
        "                continue\n",
        "            x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n",
        "            y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n",
        "            rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1: Confidence-0.7192611694335938\t\tCentre-[133.94481742 173.45008862] Height,Width-[44.48038623 44.38547942]\n",
            "1: Confidence-0.7321313619613647\t\tCentre-[145.78119493 180.25489563] Height,Width-[30.22330516 49.00654077]\n",
            "1: Confidence-0.7364127039909363\t\tCentre-[146.09604633 180.00329108] Height,Width-[40.69682261 40.27306267]\n",
            "1: Confidence-0.7331182360649109\t\tCentre-[140.29965976 186.49011993] Height,Width-[44.82474217 33.86878242]\n",
            "1: Confidence-0.8591009974479675\t\tCentre-[133.02950478 174.16802812] Height,Width-[45.14272162 40.66705217]\n",
            "1: Confidence-0.9407851696014404\t\tCentre-[139.49104172 185.90445304] Height,Width-[38.71506902 39.2043595 ]\n",
            "1: Confidence-0.9467900395393372\t\tCentre-[139.36976701 178.49921417] Height,Width-[44.64108023 40.39098825]\n",
            "1: Confidence-0.9552264213562012\t\tCentre-[138.36097181 177.84572363] Height,Width-[45.32551524 44.85514113]\n",
            "1: Confidence-0.9760034084320068\t\tCentre-[139.49357289 174.66210985] Height,Width-[45.13726756 39.2723855 ]\n",
            "1: Confidence-0.9136365056037903\t\tCentre-[137.90209985 174.20385242] Height,Width-[45.0771918  44.75406882]\n",
            "4\n",
            "4: Confidence-0.9940075278282166\t\tCentre-[ 40.82206535 183.58270311] Height,Width-[41.37531277 41.93951365]\n",
            "4: Confidence-0.9945154786109924\t\tCentre-[ 42.45684004 184.20312238] Height,Width-[42.4848834  38.36190691]\n",
            "4: Confidence-0.9992139935493469\t\tCentre-[ 40.25707126 182.22441435] Height,Width-[42.78426215 42.24995467]\n",
            "4: Confidence-0.9999313354492188\t\tCentre-[ 39.47838545 182.08390284] Height,Width-[48.49759814 48.28816649]\n",
            "4: Confidence-0.9995155930519104\t\tCentre-[ 38.9877553  182.42548943] Height,Width-[44.04714057 42.6157998 ]\n",
            "4: Confidence-0.9995757341384888\t\tCentre-[ 41.81570363 181.92977905] Height,Width-[41.72098133 39.37939157]\n",
            "4: Confidence-0.99979168176651\t\tCentre-[ 38.228405   182.99391031] Height,Width-[43.88818949 44.5069684 ]\n",
            "4: Confidence-0.9998756051063538\t\tCentre-[ 38.94061327 181.93224669] Height,Width-[46.33029696 46.08820101]\n",
            "4: Confidence-0.9982506632804871\t\tCentre-[ 38.18386197 184.54562092] Height,Width-[32.26285117 45.43261134]\n",
            "4: Confidence-0.9996386766433716\t\tCentre-[ 39.32188869 180.47238433] Height,Width-[38.00363773 45.18526637]\n",
            "5\n",
            "5: Confidence-0.9737619161605835\t\tCentre-[131.47195452  48.29568148] Height,Width-[46.68591961 37.24041643]\n",
            "5: Confidence-0.9566197991371155\t\tCentre-[132.19767125  52.6195541 ] Height,Width-[35.55615276 52.82214224]\n",
            "5: Confidence-0.9729998111724854\t\tCentre-[125.02825785  51.77443744] Height,Width-[32.72973722 50.32369255]\n",
            "5: Confidence-0.9770208597183228\t\tCentre-[125.53402293  55.24011374] Height,Width-[47.11118826 43.49118127]\n",
            "5: Confidence-0.9800781011581421\t\tCentre-[130.38450289  55.88990211] Height,Width-[48.23894164 48.82868621]\n",
            "5: Confidence-0.9973351359367371\t\tCentre-[126.65191817  53.8118124 ] Height,Width-[49.66918322 49.14713236]\n",
            "5: Confidence-0.9936028122901917\t\tCentre-[130.62402892  54.24543834] Height,Width-[49.01314816 47.74658575]\n",
            "5: Confidence-0.9968150854110718\t\tCentre-[129.82473135  52.70680743] Height,Width-[52.81464526 52.83867976]\n",
            "5: Confidence-0.9785142540931702\t\tCentre-[131.16372627  55.50993729] Height,Width-[48.19935688 43.19594087]\n",
            "5: Confidence-0.9795491695404053\t\tCentre-[124.60952944  56.54255366] Height,Width-[47.84066149 48.45068976]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}