{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2+SSD_v6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23wuE1X4ZbLe",
        "outputId": "f66fd2a7-261d-4a46-9133-7f823cf2b51b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVv9a9uMZhj5",
        "outputId": "2ccda700-ee90-4759-c45b-b1cc40b28718"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CreateDataset.ipynb                       MobileNetV2+SSD_v2.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                                     MobileNetV2+SSD_v3.ipynb\n",
            "mnist_obj_detection_100_test.tfrecords    MobileNetV2+SSD_v4.ipynb\n",
            "mnist_obj_detection_2000_train.tfrecords  MobileNetV2+SSD_v5.ipynb\n",
            "MobileNetV2+SSD.ipynb                     MobileNetV2+SSD_v6.ipynb\n",
            "MobileNetV2+SSD_v1.ipynb                  TestKeras.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXuQP38aFi94"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy import matlib\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jL765qOfwkR"
      },
      "source": [
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "layerWidths = [28,14,7,4,2,1]\n",
        "numBoxes = [3,3,3,3,3,3]\n",
        "assert len(numBoxes) == len(layerWidths)        # numBoxes for each layer and each layer has a specific width\n",
        "outputChannels = NUM_CLASSES + 1 + 4            # 10 classes + background + cx,cy,h,w\n",
        "assert outputChannels - NUM_CLASSES == 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGtGIEkvQkFT"
      },
      "source": [
        "Reference is taken from this blog. https://d2l.ai/chapter_computer-vision/anchor.html \\\\\n",
        "\"*Assume that the input image has a height of h and width of w. We generate anchor boxes with different shapes centered on each pixel of the image. Assume the size is s∈(0,1], the aspect ratio is r>0, and the width and height of the anchor box are ws√r and hs/√r, respectively. When the center position is given, an anchor box with known width and height is determined.*\" \\\\\n",
        "\n",
        "s: scale, h: grid_size, w: grid_size and r: asp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrXPAnHPHas5"
      },
      "source": [
        "def create_default_boxes():\n",
        "\t# number of scales is equal to the number of different resolutions ie num of layer widths\n",
        "\t# for a given resolution, we have different aspect ratios\n",
        "\t# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\n",
        "\tMinScale = .1 \t\t\t\t\t# Min and Max scale given as percentage\n",
        "\tMaxScale = 1.5\n",
        "\tscales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths))]\n",
        "\tscales = scales[::-1] \t\t\t\t\t\t# reversing the order because the layer_widths go from high to low (lower to higher resoltuion)\n",
        "\n",
        "\tasp = [0.5,1.0,1.5]\n",
        "\tasp1 = [x**0.5 for x in asp]\n",
        "\tasp2 = [1/x for x in asp1]\n",
        "\n",
        "\t# Should be equal to the output of the MobileNetV2-SSD model.\n",
        "\tTOTAL_BOXES = sum([a*a*b for a,b in zip(layerWidths, numBoxes)])\t\t# Computes total number of boxes. (Sum of (layer_widths*layer_widths*num_boxes))\n",
        "\n",
        "\tcentres = np.zeros((TOTAL_BOXES,2))\n",
        "\thw = np.zeros((TOTAL_BOXES,2))\n",
        "\tboxes = np.zeros((TOTAL_BOXES,4))\n",
        "\n",
        "\t# Calculating the default boxes (centres, height, width)\n",
        "\tidx = 0\n",
        "\tfor grid_size, num_box, scale in zip(layerWidths, numBoxes, scales):\n",
        "\t\tstep_size = IMG_SIZE*1.0/grid_size\n",
        "\t\tfor i in range(grid_size):\n",
        "\t\t\tfor j in range(grid_size):\n",
        "\t\t\t\tpos = idx + (i*grid_size+j) * num_box\n",
        "\t\t\t\t# centre is the same for all aspect ratios(=num_box)\n",
        "\t\t\t\tcentres[ pos : pos + num_box , :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
        "\t\t\t\t# height and width vary according to the scale and aspect ratio\n",
        "\t\t\t\thw[ pos : pos + num_box , :] = np.multiply(grid_size*scale, np.squeeze(np.dstack([asp1, asp2]),axis=0))[:num_box,:]\n",
        "\n",
        "\t\tidx += grid_size*grid_size*num_box\n",
        "\n",
        "\n",
        "\t# (x,y) co-ordinates of top left and bottom right\n",
        "\t# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
        "\tboxes[:,0] = centres[:,0] - hw[:,0]/2\n",
        "\tboxes[:,1] = centres[:,1] - hw[:,1]/2\n",
        "\tboxes[:,2] = centres[:,0] + hw[:,0]/2\n",
        "\tboxes[:,3] = centres[:,1] + hw[:,1]/2\n",
        "\n",
        "\treturn boxes, TOTAL_BOXES, centres, hw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSzu-ma6gK-x"
      },
      "source": [
        "boxes, TOTAL_BOXES, centres, hw = create_default_boxes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLSYWbvh2kJ"
      },
      "source": [
        "# calculate IoU for a set of search boxes and default boxes\n",
        "def IoU(box1, box2):\n",
        "\tbox1 = box1.astype(np.float64)\n",
        "\tbox2 = box2.astype(np.float64)\n",
        "\n",
        "\tx_top_left = np.maximum(box1[:,0], box2[:,0])\t\t\t# find x-coordinate of top-left corner for intersection.\n",
        "\tx_bottom_right = np.minimum(box1[:,2], box2[:,2])\t\t\t# find x-cordinate of bottom-right corner for intersection.\n",
        "\ty_top_left = np.maximum(box1[:,1], box2[:,1])\t\t\t# find y-coordinate of top-left corner for intersection.\n",
        "\ty_bottom_right = np.minimum(box1[:,3], box2[:,3])\t\t\t# find y-coordinate of bottom-right corner for intersection.\n",
        "\n",
        "\tintersection = np.abs(np.maximum(x_bottom_right - x_top_left,0) * np.maximum(y_bottom_right - y_top_left,0))\n",
        "\t\n",
        "\tboxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
        "\tboxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
        "\t\n",
        "\tunionArea = boxArea1 + boxArea2 - intersection\n",
        "\tassert (unionArea > 0).all()\n",
        "\treturn intersection / unionArea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z10Q5use7to"
      },
      "source": [
        "# give the index of the box correpsonding to the IoUs > threshold (=0.5) \n",
        "def bestIoU(searchBox):\n",
        "    return np.argwhere(IoU(matlib.repmat(searchBox, TOTAL_BOXES, 1), boxes) > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXwFqot5PPse"
      },
      "source": [
        "classes = 15    # 10 classes + 1 background + 4 locations\n",
        "base_model=tf.keras.applications.MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)        # Use of MobileNetV2 as a backbone for SSD.\n",
        "\n",
        "feature0 = base_model.get_layer('block_6_expand_relu').output\n",
        "feature1 = base_model.get_layer('block_13_expand_relu').output\n",
        "\n",
        "featureMaps = 6\n",
        "features = [None for _ in range(featureMaps)]\n",
        "classifiers = [None for _ in range(featureMaps)]\n",
        "\n",
        "conv1_1 = tf.keras.layers.Conv2D(256,1,name='SSD_conv_1_1')\n",
        "conv1_2 = tf.keras.layers.Conv2D(512,3,strides=(2,2),padding='same',name='SSD_conv_1_2')\n",
        "\n",
        "conv2_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_2_1')\n",
        "conv2_2 = tf.keras.layers.Conv2D(256,3,strides=(2,2),padding='same',name='SSD_conv_2_2')\n",
        "\n",
        "conv3_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_3_1')\n",
        "conv3_2 = tf.keras.layers.Conv2D(256,3,strides=(1,1),name='SSD_conv_3_2')\n",
        "\n",
        "conv4_1 = tf.keras.layers.Conv2D(128,1,name='SSD_conv_4_1')\n",
        "conv4_2 = tf.keras.layers.Conv2D(256,2,strides=(1,1),name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n",
        "\n",
        "conv = []\n",
        "reshape = []\n",
        "\n",
        "for i in range(featureMaps):\n",
        "    conv.append(tf.keras.layers.Conv2D(numBoxes[i]*outputChannels, 3, padding='same', name='Classification_'+str(i)))\n",
        "    reshape.append(tf.keras.layers.Reshape((layerWidths[i]* layerWidths[i] * numBoxes[i], outputChannels),name='Reshape_classification_'+str(i)))\n",
        "\n",
        "# Use of different spatial features.\n",
        "features[0] = feature0\n",
        "features[1] = feature1\n",
        "features[2] = conv1_2(conv1_1(features[1]))\n",
        "features[3] = conv2_2(conv2_1(features[2]))\n",
        "features[4] = conv3_2(conv3_1(features[3]))\n",
        "features[5] = conv4_2(conv4_1(features[4]))\n",
        "\n",
        "for i in range(featureMaps):\n",
        "    x = conv[i](features[i])            # Apply 1x1 convolutions to generate bounding boxes.\n",
        "    x = reshape[i](x)                   # Reshape the output to (batch_size x TOTAL_BOXES x classes)\n",
        "    classifiers[i] = x\n",
        "output = tf.keras.layers.concatenate(classifiers, axis = -2, name='concatenate')\n",
        "\n",
        "model=tf.keras.Model(inputs=base_model.input,outputs=output) #specify the inputs and outputs\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDaTtWagzW6B"
      },
      "source": [
        "Following functions are used to access the dataset from tfrecords. (Used https://dzlab.github.io/dltips/en/tensorflow/tfrecord/ as a reference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiTFiYyRFNM"
      },
      "source": [
        "def _parse_function(proto):\n",
        "\t# define your tfrecord again. Remember that you saved your image as a string.\n",
        "\tkeys_to_features = {'image': tf.io.FixedLenFeature([], tf.string),\n",
        "\t\t\t\t\t\t'label': tf.io.FixedLenFeature([], tf.string)}\n",
        "\t\n",
        "\t# Load one example\n",
        "\tparsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
        "\t\n",
        "\t# Turn your saved image string into an array\n",
        "\tparsed_features['image'] = tf.io.decode_raw(parsed_features['image'], tf.float64)\n",
        "\tparsed_features['image'] = tf.reshape(parsed_features['image'], (224, 224, 3))\n",
        "\tparsed_features['label'] = tf.io.decode_raw(parsed_features['label'], tf.float64)\n",
        "\tparsed_features['label'] = tf.reshape(parsed_features['label'], (3150, 5))\n",
        "\n",
        "\treturn parsed_features['image'], parsed_features['label']\n",
        "\n",
        "  \n",
        "def create_dataset(filepath):\n",
        "\t\n",
        "\t# This works with arrays as well\n",
        "\tdataset = tf.data.TFRecordDataset(filepath)\n",
        "\t\n",
        "\t# Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
        "\tdataset = dataset.map(_parse_function, num_parallel_calls=8)\n",
        "\treturn dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6mXt7Y7Y9tu"
      },
      "source": [
        "train_dataset = create_dataset(['data/mnist_obj_detection_2000_train.tfrecords'])\n",
        "test_dataset = create_dataset(['data/mnist_obj_detection_100_test.tfrecords'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM2jG3BoRJKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f575b0-0a54-48fa-d45a-62d5eb05b672"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "SHUFFLE_BUFFER_SIZE = 60\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((10, 224, 224, 3), (10, 3150, 5)), types: (tf.float64, tf.float64)>\n",
            "<BatchDataset shapes: ((10, 224, 224, 3), (10, 3150, 5)), types: (tf.float64, tf.float64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOV-HjtxyhU-"
      },
      "source": [
        "Smooth L1 loss function's code is based on the function given at https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDHTUkF6RMLb"
      },
      "source": [
        "# calculate the smooth L1 loss\n",
        "def smoothL1(x,y, label):\n",
        "    diff = K.abs(x-y)\n",
        "    result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
        "    indicator_fn = tf.cast(label < 10, tf.float32)\n",
        "    result = result*tf.expand_dims(indicator_fn, axis=2)\n",
        "    return K.mean(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwBzQGiAwty0"
      },
      "source": [
        "Below function for hard negative mining is referred from this repository\n",
        "https://github.com/ChunML/ssd-tf2/blob/master/losses.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzEBHOsK7TLg"
      },
      "source": [
        "# This is the old code for negative hard mining.\n",
        "# def hard_negative_mining(loss, gt_confs, neg_ratio=3):\n",
        "#     pos_idx = gt_confs < 10\n",
        "#     num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
        "#     num_neg = num_pos * neg_ratio\n",
        "\n",
        "#     rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
        "#     rank = tf.argsort(rank, axis=1)\n",
        "#     neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
        "\n",
        "#     return pos_idx, neg_idx\n",
        "\n",
        "# def confidenceLoss(y, label):\n",
        "#     classification_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
        "#     pos_idx, neg_idx = hard_negative_mining(classification_loss, label)\n",
        "#     cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "#     classification_loss = cross_entropy(label[tf.math.logical_or(pos_idx, neg_idx)], y[tf.math.logical_or(pos_idx, neg_idx)])\n",
        "#     return K.mean(classification_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crs0T3_O9Lc5"
      },
      "source": [
        "The following loss function is written based on the information given in this blog (https://becominghuman.ai/tensorflow-object-detection-api-basics-of-detection-7b134d689c75)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBgwXQ8dTUTp"
      },
      "source": [
        "# Contains implementation of Hard Negative Mining.\n",
        "def confidenceLoss(y, label):\n",
        "    total_classification_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
        "\n",
        "    positive_indices = label < 10                                           # Find boxes having mnist digits.\n",
        "    positive_classification_loss = tf.math.reduce_mean(total_classification_loss[positive_indices])\n",
        "\n",
        "    negative_indices = 1.0 - tf.cast(positive_indices, tf.float32)                                 # Find boxes having no mnist digits.\n",
        "    negative_classification_loss = total_classification_loss*negative_indices                      # Make loss of all postive boxes as zero.\n",
        "    negative_classification_loss = tf.sort(negative_classification_loss, axis=1, direction='DESCENDING')\n",
        "    \n",
        "    positive_indices = tf.cast(positive_indices, tf.int32)                  # Cast the boolean tensor to integers.\n",
        "    total_num_positives = tf.math.reduce_sum(positive_indices, axis=1)      # Find the total number of boxes containing mnist digits.\n",
        "    required_num_negatives = tf.cast(3*total_num_positives, tf.int32)       # Usual ratio of negative examples to positive examples is 3:1. (This will give value of k to choose top-k negative losses.)\n",
        "    \n",
        "    batch_size, number_images = negative_classification_loss.shape          # Find batch size and number of image.\n",
        "    array = tf.constant(np.array([np.arange(number_images) for _ in range(batch_size)]))        # Create an array to select top-k negative losses.\n",
        "    array = tf.cast(array, tf.int32)\n",
        "    negative_indices = array < tf.expand_dims(required_num_negatives, axis=1)                               # Make True for top-k elements and the rest to be False.\n",
        "    negative_classification_loss = tf.math.reduce_mean(negative_classification_loss[negative_indices])       # Compute negative loss using top-k elements.\n",
        "\n",
        "    return tf.math.reduce_mean([positive_classification_loss, negative_classification_loss])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byz853l8RayD"
      },
      "source": [
        "def Loss(gt, y):\n",
        "    # shape of y is batch_size * BOXES * outputChannels\n",
        "    # shape of gt is batch_size * BOXES * 5 \n",
        "    loss = 0\n",
        "    # localisation loss\n",
        "    loss += smoothL1(y[:,:,-4:], gt[:,:,-4:], gt[:, :, 0])\n",
        "    # confidence loss\n",
        "    loss += confidenceLoss(y[:,:,:-4], tf.cast(gt[:,:,0],tf.int32))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU4XVmr2Rg4Z"
      },
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),loss=Loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJmRwVCRkYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03af56c-a313-4291-dad8-18b8d9546677"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=20, validation_data = test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "200/200 [==============================] - 47s 158ms/step - loss: 2.5013 - val_loss: 1.5785\n",
            "Epoch 2/20\n",
            "200/200 [==============================] - 36s 167ms/step - loss: 1.1459 - val_loss: 1.2937\n",
            "Epoch 3/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.8479 - val_loss: 1.1122\n",
            "Epoch 4/20\n",
            "200/200 [==============================] - 19s 94ms/step - loss: 0.6908 - val_loss: 0.9598\n",
            "Epoch 5/20\n",
            "200/200 [==============================] - 19s 93ms/step - loss: 0.5905 - val_loss: 0.9155\n",
            "Epoch 6/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.5349 - val_loss: 0.8924\n",
            "Epoch 7/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.4884 - val_loss: 0.7978\n",
            "Epoch 8/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.4522 - val_loss: 0.7146\n",
            "Epoch 9/20\n",
            "200/200 [==============================] - 19s 91ms/step - loss: 0.4230 - val_loss: 0.6957\n",
            "Epoch 10/20\n",
            "200/200 [==============================] - 19s 91ms/step - loss: 0.4041 - val_loss: 0.6058\n",
            "Epoch 11/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.3820 - val_loss: 0.6145\n",
            "Epoch 12/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.3635 - val_loss: 0.5454\n",
            "Epoch 13/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.3475 - val_loss: 0.5474\n",
            "Epoch 14/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.3354 - val_loss: 0.5678\n",
            "Epoch 15/20\n",
            "200/200 [==============================] - 19s 91ms/step - loss: 0.3193 - val_loss: 0.5738\n",
            "Epoch 16/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.3113 - val_loss: 0.5374\n",
            "Epoch 17/20\n",
            "200/200 [==============================] - 19s 91ms/step - loss: 0.3003 - val_loss: 0.5214\n",
            "Epoch 18/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.2888 - val_loss: 0.5460\n",
            "Epoch 19/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.2836 - val_loss: 0.5207\n",
            "Epoch 20/20\n",
            "200/200 [==============================] - 19s 92ms/step - loss: 0.2732 - val_loss: 0.4746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYujGKBxM47V",
        "outputId": "3358f146-44bb-4013-d4f4-64196e3633c2"
      },
      "source": [
        "# get prediction for one sample\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(test_dataset)\n",
        "image, label = iterator.next()\n",
        "y_pred = model.predict(image)\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3150, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmnRvhMhyGXj"
      },
      "source": [
        "Below visulization codes are referred from other repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBMo0v3VM9Xr"
      },
      "source": [
        "import bottleneck\n",
        "OBJperCLASS = 10 # get the top 10 results for each class\n",
        "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
        "def infer(Y):\n",
        "    # classes are actually the index into the default boxes\n",
        "    classes = np.zeros((OBJperCLASS, outputChannels-4), dtype=np.uint16)        # [10, 11]\n",
        "    conf = np.zeros((OBJperCLASS, outputChannels-4))                            # [10, 11]\n",
        "    delta = np.zeros((OBJperCLASS, outputChannels-4, 4))                        # [10, 11, 4]\n",
        "    class_predictions = softmax(Y[:,:outputChannels-4],axis=1)                  # [3150, 11]\n",
        "    for i in range(outputChannels-4):                                           # (Loop to find boxes for each mnist digit class)\n",
        "        # Basically the below line is used to find 11 boxes for the present mnist digit.\n",
        "        classes[:,i] = bottleneck.argpartition(class_predictions[:,i], TOTAL_BOXES-1-10 , axis=-1)[-OBJperCLASS:]      # It will take class_prediction[:, i] (sized 3150,1) and argpartition at the 3150-11th element from last.\n",
        "        conf[:,i] = class_predictions[classes[:,i],i]               # Confidence score for each digit.\n",
        "        delta[:,i] = Y[classes[:,i],outputChannels-4:]              # Used to find cx, cy, h, w.\n",
        "    return conf, classes, delta\n",
        "\n",
        "# generate bounding boxes from the inferred outputs\n",
        "def Bbox(confidence,box_idx,delta):\n",
        "    #delta contains delta(cx,cy,h,w)\n",
        "    bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
        "    bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
        "    for i in range(OBJperCLASS):\n",
        "        bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
        "        bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
        "        bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
        "        bbox_hw[i,:,1] = hw[box_idx[i]][:,1] + delta[i,:,3]\n",
        "    return bbox_centre,bbox_hw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aWsKge3E5oW"
      },
      "source": [
        "Reference for algorithm of non-maximum-supression is referred from https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5eg0OM7yfWU"
      },
      "source": [
        "def bbox_centre2corner(bbox_centre, bbox_hw):\n",
        "    box = np.zeros((1,4))\n",
        "    box[0, 0] = bbox_centre[0] - bbox_hw[0]/2\n",
        "    box[0, 1] = bbox_centre[1] - bbox_hw[1]/2\n",
        "    box[0, 2] = bbox_centre[0] + bbox_hw[0]/2\n",
        "    box[0, 3] = bbox_centre[1] + bbox_hw[1]/2\n",
        "    return box\n",
        "\n",
        "def non_maximum_supression(bbox_centre, bbox_hw, confidence, threshold):\n",
        "    result_bbox_c = []\n",
        "    result_bbox_hw = []\n",
        "    result_confidence = []\n",
        "    count = 0\n",
        "    while np.size(bbox_centre) != 0:\n",
        "        argmax = np.argsort(confidence)[-1]\n",
        "        result_bbox_c.append(bbox_centre[argmax])\n",
        "        result_bbox_hw.append(bbox_hw[argmax])\n",
        "        result_confidence.append(confidence[argmax])\n",
        "\n",
        "        bbox_centre = np.delete(bbox_centre, argmax, 0)\n",
        "        bbox_hw = np.delete(bbox_hw, argmax, 0)\n",
        "        confidence = np.delete(confidence, argmax, 0)\n",
        "\n",
        "        remove_idxs = []\n",
        "        for k in range(bbox_centre.shape[0]):\n",
        "            box1 = bbox_centre2corner(result_bbox_c[count], result_bbox_hw[count])\n",
        "            box2 = bbox_centre2corner(bbox_centre[k], bbox_hw[k])\n",
        "            iou = IoU(box1, box2)\n",
        "            if iou>threshold:\n",
        "                remove_idxs.append(k)\n",
        "        bbox_centre = np.delete(bbox_centre, remove_idxs, 0)\n",
        "        bbox_hw = np.delete(bbox_hw, remove_idxs, 0)\n",
        "        confidence = np.delete(confidence, remove_idxs, 0)\n",
        "        count += 1\n",
        "    return np.array(result_bbox_c).reshape(count, 2), np.array(result_bbox_hw).reshape(count, 2), np.array(result_confidence).reshape(count,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "KeclD8VkNAJr",
        "outputId": "8cb76043-8a97-4969-d680-6d37fb5d966e"
      },
      "source": [
        "idx = np.random.randint(BATCH_SIZE)\n",
        "from scipy.special import softmax\n",
        "\n",
        "# top 10 predictions for each class\n",
        "confidence, box_idx, delta = infer(y_pred[idx])\n",
        "bbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n",
        "\n",
        "im = np.array(Image.fromarray(image[idx].numpy().astype(np.uint8)))\n",
        "fig,ax = plt.subplots(1)\n",
        "ax.imshow(im)\n",
        "\n",
        "for i in range(outputChannels-4):\n",
        "  # skipping backgrounds\n",
        "    if i == NUM_CLASSES:\n",
        "        continue\n",
        "    color = 'r'\n",
        "    # if a class is mentioned in the ground truth, color the boxes green\n",
        "    if i in label[idx,:,0]:\n",
        "        color = 'g'\n",
        "    \n",
        "    if (confidence[:,i]>0.5).any() or i in label[idx, :, 0]:\n",
        "        result_bbox_c, result_bbox_hw, result_confidence = non_maximum_supression(bbox_centre[:,i], bbox_hw[:,i], confidence[:,i], 0.4)\n",
        "        for ii in range(result_bbox_c.shape[0]):\n",
        "            print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,result_confidence[ii],result_bbox_c[ii],result_bbox_hw[ii]))\n",
        "            x = result_bbox_c[ii,0] - result_bbox_hw[ii,0]/2\n",
        "            y = result_bbox_c[ii,1] - result_bbox_hw[ii,1]/2\n",
        "            rect = patches.Rectangle((y,x),result_bbox_hw[ii,1],result_bbox_hw[ii,0],linewidth=1,edgecolor=color,facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "    \n",
        "    # # skip all the classes which have low confidence values\n",
        "    # if (confidence[:,i] > 0.5).any() or i in label[idx,:,0]:\n",
        "    #     for k in range(OBJperCLASS):\n",
        "    #         print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n",
        "      \n",
        "    #         # draw bounding box only if confidence scores are high\n",
        "    #         if confidence[k,i] < 0.5:\n",
        "    #             continue\n",
        "    #         x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n",
        "    #         y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n",
        "    #         rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n",
        "    #         ax.add_patch(rect)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2: Confidence-[0.47390249]\t\tCentre-[105.25201201 103.28097129] Height,Width-[26.08915183 25.88528582]\n",
            "5: Confidence-[0.99986792]\t\tCentre-[ 87.89479375 178.8705461 ] Height,Width-[39.83817062 41.5327691 ]\n",
            "6: Confidence-[0.99992919]\t\tCentre-[ 72.16457582 122.57190454] Height,Width-[56.35141513 54.90089747]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}